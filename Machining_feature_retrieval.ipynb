{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the necessary libraries\n",
    "\n",
    "import os\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "import open3d as o3d\n",
    "import matplotlib.pyplot as plt\n",
    "from model.resunet import ResUNetBN2C\n",
    "from util.visualization import get_colored_point_cloud_feature\n",
    "from numpy import linalg as LA\n",
    "from util.feature_extraction import stl2ply_convert, feature_extract\n",
    "from util.misc import sort_list, zero_pad\n",
    "from util.spp_layer import  SpatialPyramidPooling\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from tensorflow.python.keras.layers import Layer\n",
    "import tensorflow.keras.backend as K\n",
    "K.image_data_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpickling the features previously extracted\n",
    "\n",
    "with open(\"data/names_paths_features_featurenet.dat\", \"rb\") as f:\n",
    "    file_names, file_paths, features = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the number of classes and the data count\n",
    "families = Counter(file_names)\n",
    "print(\"Number of classes:\",len(families))\n",
    "print(\"\\n\",families)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the distrubtion of feaure dimensions along all the 24 classes\n",
    "shape_list = list()\n",
    "for i in range(0,len(features)):\n",
    "    f = features[i]\n",
    "    shape_list.append(f.shape[0])\n",
    "xs = [x for x in range(len(shape_list))]\n",
    "plt.plot(xs,shape_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking maximum feature dimension\n",
    "max_val=0\n",
    "\n",
    "for i in range(0,len(features)):\n",
    "    f = features[i]\n",
    "    max = np.maximum(f.shape[0],f.shape[1])\n",
    "    if(max > max_val):\n",
    "        max_val = max\n",
    "print(max_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero padding all the vectors to max value\n",
    "def zero_pad(x):\n",
    "    shape_pad = max_val \n",
    "    x1 = np.zeros([shape_pad,32])\n",
    "    x1[:x.shape[0],:x.shape[1]] = x\n",
    "    return x1\n",
    "max_val = 240\n",
    "for j in range(0,len(features)):\n",
    "    z = zero_pad(features[j])\n",
    "    features[j] =z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "X = np.asarray(features)\n",
    "\n",
    "# one-hot encoding\n",
    "Y_set = set(file_names)\n",
    "Y_list = (list(Y_set))\n",
    "mapping = {}\n",
    "family = list()\n",
    "\n",
    "# 24 unique features\n",
    "for x in range(len(Y_list)):\n",
    "  mapping[Y_list[x]] = x\n",
    "\n",
    "# mapping to corresponding category\n",
    "for x in range(len(file_names)):\n",
    "  family.append(mapping[file_names[x]])\n",
    "\n",
    "Y = to_categorical(family)\n",
    "Y = np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=1)\n",
    "x_test, x_val, y_test, y_val = train_test_split(x_test, y_test, test_size=0.5, random_state=1)\n",
    "# val,test = 0.5 x 0.3 = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convering to tensors\n",
    "x_train = tf.convert_to_tensor(x_train, dtype=None, dtype_hint=None, name=None)\n",
    "x_val = tf.convert_to_tensor(x_val, dtype=None, dtype_hint=None, name=None)\n",
    "x_test = tf.convert_to_tensor(x_test, dtype=None, dtype_hint=None, name=None)\n",
    "y_train = tf.convert_to_tensor(y_train, dtype=None, dtype_hint=None, name=None)\n",
    "y_val = tf.convert_to_tensor(y_val, dtype=None, dtype_hint=None, name=None)\n",
    "y_test = tf.convert_to_tensor(y_test, dtype=None, dtype_hint=None, name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping tensor for input to neural net\n",
    "\n",
    "x_train = tf.reshape(x_train, [len(x_train),max_val,32,1])\n",
    "x_val = tf.reshape(x_val, [len(x_val),max_val,32,1])\n",
    "x_test = tf.reshape(x_test, [len(x_test),max_val,32,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Convolution2D, Activation, MaxPooling2D, Dense, Dropout\n",
    "\n",
    "\n",
    "# Minimizes Tensorflow Logging\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "NUM_CHANNELS = 1\n",
    "NUM_CLASSES = 24\n",
    "\n",
    "def makeModel():\n",
    "    model = Sequential()\n",
    "\n",
    "    # MODEL 1\n",
    "    # uses tensorflow ordering. Note that we leave the image size as None to allow multiple image sizes\n",
    "    model.add(Convolution2D(32, 3, NUM_CHANNELS, padding='same', input_shape=(max_val,32,1)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Convolution2D(32, 3, NUM_CHANNELS, padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(64, 3, NUM_CHANNELS, padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Convolution2D(64, 3, NUM_CHANNELS, padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(SpatialPyramidPooling([1, 2, 4]))\n",
    "    model.add(Dense(NUM_CLASSES))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.executing_eagerly = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model=makeModel()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running and Saving model\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "filepath = \"model/pretrained/spp_featurenet.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1,\n",
    "                             save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "history = model.fit(x_train, y_train, batch_size=1, epochs= 30, verbose=2,\n",
    "                    validation_data=(x_val,y_val), shuffle=True, class_weight=None,\n",
    "                    sample_weight=None, initial_epoch=0, steps_per_epoch=None, \n",
    "                    validation_steps=None, validation_freq=1, max_queue_size=10,\n",
    "                    workers=1,callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the model\n",
    "\n",
    "from  tensorflow.keras.models import load_model\n",
    "model = load_model(\"model/pretrained/spp_featurenet.h5\",custom_objects={'SpatialPyramidPooling': SpatialPyramidPooling})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a temperory model for spp output\n",
    "\n",
    "from  tensorflow.keras.models import Model\n",
    "layer_name = 'my_layer'\n",
    "intermediate_layer_model = Model(inputs=model.input,outputs=model.get_layer('spatial_pyramid_pooling').output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding accuracy on test set\n",
    "spp_train = intermediate_layer_model.predict(x_train)\n",
    "\n",
    "top5_acc =0\n",
    "top1_acc =0\n",
    "top5_lbl = list()\n",
    "sim_list = list()\n",
    "null_index = list()\n",
    "classes = 24\n",
    "\n",
    "for i in tqdm (range(0,len(x_test))):\n",
    "    sim_list.clear()\n",
    "    #test_feat = spp_test[i]\n",
    "    test_feat = tf.reshape(x_test[i],[1,max_val,32,1])\n",
    "    spp_test = intermediate_layer_model.predict(test_feat)\n",
    "    y_t = y_test[i]\n",
    "    for j in range(0,classes):\n",
    "        if(y_t[j].numpy()==1.0):\n",
    "            test_lbl = Y_list[j]  \n",
    "    for k in range(0,len(x_train)):  \n",
    "        #train_feat = tf.reshape(x_train[i],[1,max_val,32,1])\n",
    "        sim_list.append(abs(np.linalg.norm(spp_train[k]-spp_test)))\n",
    "    id = list(range(0,len(x_train)))\n",
    "    Sim_models_id = [x for _,x in sorted(zip(sim_list,id))]\n",
    "    top5 = (Sim_models_id[0:5])\n",
    "    top5_lbl.clear()\n",
    "    for l in range(0,5): \n",
    "        yi = y_train[top5[l]]\n",
    "        for m in range(0,classes):\n",
    "            if ((yi[m].numpy())==1.0):\n",
    "                top5_lbl.append(Y_list[m])\n",
    "    if(test_lbl in top5_lbl):\n",
    "        top5_acc+=1\n",
    "        \n",
    "    if (top5_lbl):\n",
    "        if(test_lbl == top5_lbl[0]):\n",
    "            top1_acc+=1\n",
    "    else:\n",
    "        null_index.append(i)\n",
    "        \n",
    "print(\"Accuracy for \",len(x_test),\"testing files is \",top1_acc/len(x_test))\n",
    "print(\"Top 5 accuracy for \",len(x_test),\"testing files is \",top5_acc/len(x_test))\n",
    "print(len(null_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving similar features from the dataset for a sample file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading stl dataset paths \n",
    "\n",
    "db_folder = \"data/stl\"\n",
    "os.path.abspath(db_folder)\n",
    "ind = 0\n",
    "stl_file_path = list()\n",
    "sub_folders = os.listdir(db_folder)\n",
    "for sub_folder in sub_folders:\n",
    "        sub_folder_path = os.path.join(db_folder, sub_folder)\n",
    "        stl_files = os.listdir(sub_folder_path)\n",
    "        for stl_file in stl_files:\n",
    "            if stl_file.endswith(\".STL\"):\n",
    "                stl_file_path.append(os.path.join(sub_folder_path, stl_file))\n",
    "                ind+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spp_out(feat):\n",
    "    test_feat = tf.reshape(feat,[1,max_val,32,1])\n",
    "    spp_test = intermediate_layer_model.predict(test_feat)\n",
    "    return spp_test    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test file\n",
    "\n",
    "test_id = 1990\n",
    "print(\"Test file\\n\",\"\\nID:\\t\",test_id,\"\\tFamily:\\t\",file_names[test_id])\n",
    "test_feat = zero_pad(features[test_id])\n",
    "spp_test_feat = get_spp_out(test_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comparing similaity between one test file and all the features individually\n",
    "\n",
    "n_files = len(features)\n",
    "\n",
    "sim_list = list()\n",
    "for i in range(0,n_files):\n",
    "    if(i!=test_id):\n",
    "        feat = zero_pad(features[i])\n",
    "        feat = tf.reshape(feat,[1,max_val,32,1])\n",
    "        spp_feat = intermediate_layer_model.predict(feat)\n",
    "        sim_list.append(abs(np.linalg.norm(spp_feat - spp_test_feat)))\n",
    "    else:\n",
    "        sim_list.append(float(\"inf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = list(range(0,n_files))\n",
    "\n",
    "Similar_models_id = [x for _,x in sorted(zip(sim_list,id))]\n",
    "top_5 = (Similar_models_id[0:5])\n",
    "top_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualiztion of the CAD files\n",
    "from solid import*\n",
    "import viewscad\n",
    "r = viewscad.Renderer()\n",
    "print(\"Test file\\n\",\"\\nID:\\t\",test_id,\"\\tFamily:\\t\",file_names[test_id])\n",
    "r.render_stl(stl_file_path[test_id])\n",
    "print(\"\\nTop-5 similar models and their IDs\\n\")\n",
    "for i in range(0,5): \n",
    "    yi = file_names[top_5[i]]\n",
    "    print(\"ID:\\t\",top_5[i],\"\\tFamily:\\t\",yi)\n",
    "    r.render_stl(stl_file_path[top_5[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "s4engine",
   "language": "python",
   "name": "s4engine"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
